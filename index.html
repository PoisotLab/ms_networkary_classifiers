<!DOCTYPE html>
<html lang="" dir="ltr">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Guidelines for the supervised learning of species interactions</title>
    <script src="https://hypothes.is/embed.js" async></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css"
    integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,300;0,400;0,500;1,300;1,400;1,500&display=swap');
        @import url('https://fonts.googleapis.com/css2?family=Roboto+Condensed:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap');
        @import url('https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,400;0,500;1,400;1,500&display=swap');

        :root {
            --citation: #1976D2;
            --structure: hsl(210, 61%, 43%);
            
            --light: #474747;
            --pale: #a5a5a5;
            --grey: #e0e0e0;
            --nav: #fdfdfd;

            --width: 820px;
            --inner-space: 1.2em;
        }

        body {
            font-family: "Roboto", sans-serif;
            margin: 0 0;
            padding: 0 0;
        }

        article > p {
            line-height: 135%;
        }
        article > h1 {
            padding-top: 3em;
        }
        a {
            text-decoration: none;
            color: #000;
            border-bottom: 1px solid var(--structure);
        }
        a:hover {
            font-weight: 500;
        }
        .citation {
            color: var(--citation);
        }
        .container {
            margin: 0em auto;
            max-width: var(--width);
            padding: var(--inner-space);
        }
        header > .container {
            margin-top: 4em;
        }
        code, pre {
            font-family: "Roboto Mono", monospace;
        }
        pre {
            white-space: pre-wrap;
        }
        h1, h2, h3 {
            color: var(--structure);
            font-family: "Roboto Condensed", sans-serif;
            font-weight: 400;
        }
        header {
            font-family: "Roboto Condensed", sans-serif;
            z-index: 5;
            color: black;
            margin-top: 4rem;
        }
        .header--title {
            font-weight: 400;
            font-size: 2rem;
            margin-bottom: 1rem;
        }
        .header--authors > span {
            color: var(--light);
            font-weight: 400;
        }
        main {
            padding-top: 8em !important;
            padding-bottom: 8em !important;
        }
        footer {
            background-color: var(--grey);
            font-weight: 300;
        }
        .abstract {
            color: var(--light);
            margin-bottom: 5em;
        }
        figure {
            max-width: 100%;
            border: 1px solid var(--pale);
            border-radius: 2px;
            margin: 1em 0.5em;
        }
        figure > img {
            max-width: 100%;
            display: block;
            margin: 0em auto;
        }
        figure > figcaption {
            padding: var(--inner-space);
        }
        table {
            width: 100%;
            min-width: 100%;
            border-collapse: collapse;
        }
        table > caption {
            padding: var(--inner-space);
        }
        figcaption, caption {
            color: var(--light);
            font-weight: 300;
        }
        thead {
            color: var(--structure);
        }
        td, th {
            padding: 3px;
        }
        tr {
            border-top: 1px solid var(--structure);
            border-bottom: 1px solid var(--structure);
        }
        tbody > tr:nth-child(even) {background: var(--grey)};
        tbody {
            border-bottom: 1px solid var(--structure);
        }
        #refs {
            font-size: 90%;
            column-count: 2;
        }
        #refs > .csl-entry {
            padding-bottom: 0.2em;
            padding-top: 0.2em;
        }
        .hanging-indent > .csl-entry {
            padding-left: 15px ;
            text-indent: -15px ;
        }    </style>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css"
    integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous" />
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js"
    integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O"
    crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js"
    integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF"
    crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false }
                ]
            });
        });
    </script>
</head>

<body>
    <header>
        <div class="container">
            <div class="header--title">Guidelines for the supervised learning of species interactions</div>
            <div class="header--authors">
                <span class="header--author">
                <a href="https://orcid.org/0000-0002-0735-5184" target="_blank">
                    Timothée Poisot
                </a>
                <sup class="affilcode">1</sup>
                <sup class="affilcode">2</sup>
                </span>
            </div>
        </div>
    </header>
    
    <main class="container">
        <section class="abstract">
            
            <ol>
                <li>The prediction of species interaction networks is gaining momentum as a way to circumvent limitations in data volume. Yet, ecological networks are challenging to predict because they are typically small and sparse. Dealing with extreme class imbalance is a challenge for most binary classifiers, and there are currently no guidelines as to how predictive models can be trained.</li>
                <li>Using simple mathematical arguments and numerical experiments in which a variety of classifiers (for supervised learning) are trained on simulated networks, we develop a series of guidelines related to the choice of measures to use for model selection, and the degree of unbiasing to apply to the training dataset.</li>
                <li>Classifier accuracy and the ROC-AUC are not informative measures for the performance of interaction prediction. PR-AUC is a fairer assesment of performance. In some cases, even standard measures can lead to selecting a more biased classifier because the effect of connectance is strong. The amount of correction to apply to the training dataset depends as a function of the classifier and the network connectance.</li>
                <li>These results reveal that training machines to predict networks is a challenging task, and that in virtually all cases, the composition of the training set needs to be experimented on before performing the actual training. We discuss these consequences in the context of the low volume of data.</li>
            </ol>

            <div class="artifacts">
                <a href="draft.pdf" title="Draft PDF" target="_blank">draft</a>
                <a href="preprint.pdf" title="Preprint PDF" target="_blank">preprint</a>
            </div>
        </section>
        
        <article>
            <p>example on diagnostic test: rare events are hard to detect even with really good models</p>
            <p>summary of model challenges for networks - <span class="citation" data-cites="Strydom2021RoaPre">Strydom et al. (2021)</span> importance of drawing on traits + validation is challenging - <span class="citation" data-cites="Whalen2021NavPit">Whalen et al. (2021)</span> machine learning from genomics</p>
            <p>Binary classifiers are usually assessed by measuring properties of their confusion matrix, <em>i.e.</em> the contingency table reporting true/false positive/negative hits. The same approach is used to evaluate <em>e.g.</em> species occurrence models <span class="citation" data-cites="Allouche2006AssAcc">(Allouche et al., 2006)</span>. A confusion matrix is laid out as</p>
            <p><span class="math display">$$\begin{pmatrix}
                \text{tp} &amp; \text{fp} \\
                \text{fn} &amp; \text{tn}
            \end{pmatrix} \,,$$</span></p>
            <p>wherein <span class="math inline">tp</span> is the number of interactions predicted as positive, <span class="math inline">tn</span> is the number of non-interactions predicted as negative, <span class="math inline">fp</span> is the number of non-interactions predicted as positive, and <span class="math inline">fn</span> is the number of interactions predicted as negative. Almost all measures based on the confusion matrix express rates of error or success as proportions, and therefore the values of these components matter in a <em>relative</em> way. At a coarse scale, a classifier is <em>accurate</em> when the trace of the matrix divided by the sum of the matrix is close to 1, with other measures focusing on different ways in which the classifer is wrong.</p>
            <p>list of problems to solve - baseline values and response to bias - effect of training set bias on performance - which models need the least amount of interactions to work</p>
            <p>summary of the results</p>
            <h1 id="baseline-values">Baseline values</h1>
            <h2 id="confusion-matrix-with-skill-and-bias">Confusion matrix with skill and bias</h2>
            <p>In this section, we will assume a network of connectance <span class="math inline"><em>ρ</em></span>, <em>i.e.</em> having <span class="math inline"><em>ρ</em><em>S</em><sup>2</sup></span> interactions (where <span class="math inline"><em>S</em></span> is the species richness), and <span class="math inline">(1 − <em>ρ</em>)<em>S</em><sup>2</sup></span> non-interactions. Therefore, the vector describing the <em>true</em> state of the network is a column vector <span class="math inline"><strong>o</strong><sup><em>T</em></sup> = [<em>ρ</em>(1 − <em>ρ</em>)]</span> (we can safely drop the <span class="math inline"><em>S</em><sup>2</sup></span> terms, as we will work on the confusion matrix, which ends up expressing <em>relative</em> values).</p>
            <p>In order to write the values of the confusion matrix for a hypothetical classifier, we need to define two characteristics: its skill, and its bias. Skill, here, refers to the propensity of the classifier to get the correct answer (<em>i.e.</em> to assign interactions where they are, and to not assign them where they are not). A no-skill classifier guesses at random, <em>i.e.</em> it will guess interactions with a probability <span class="math inline"><em>ρ</em></span>. The predictions of a no-skill classifier can be expressed as a row vector <span class="math inline"><strong>p</strong> = [<em>ρ</em>(1 − <em>ρ</em>)]</span>. The confusion matrix <span class="math inline"><strong>M</strong></span> for a no-skill classifier is given by the element-wise product of these vectors <span class="math inline"><strong>o</strong> ⊙ <strong>p</strong></span>, <em>i.e.</em></p>
            <p><span class="math display">$$
            \mathbf{M} = \begin{pmatrix}
                \rho^2 &amp; \rho (1-\rho) \\
                (1-\rho) \rho &amp; (1-\rho)^2
            \end{pmatrix} \,.
            $$</span></p>
            <p>In order to regulate the skill of this classifier, we can define a skill matrix <span class="math inline"><strong>S</strong></span> with diagonal elements equal to <span class="math inline"><em>s</em></span>, and off-diagonal elements equal to <span class="math inline">(1 − <em>s</em>)</span>, and re-express the skill-adjusted confusion matrix as <span class="math inline"><strong>M</strong> ⊙ <strong>S</strong></span>, <em>i.e.</em></p>
            <p><span class="math display">$$
            \begin{pmatrix}
                \rho^2 &amp; \rho (1-\rho) \\
                (1-\rho) \rho &amp; (1-\rho)^2
            \end{pmatrix} \odot \begin{pmatrix}
                s &amp; (1-s) \\
                (1-s) &amp; s
            \end{pmatrix} \,.
            $$</span></p>
            <p>Note that when <span class="math inline"><em>s</em> = 0</span>, <span class="math inline">Tr(<strong>M</strong>) = 0</span> (the classifier is <em>always</em> wrong), when <span class="math inline"><em>s</em> = 0.5</span>, the classifier is no-skill and guesses at random, and when <span class="math inline"><em>s</em> = 1</span>, the classifier is perfect.</p>
            <p>The second element we can adjust in this hypothetical classifier is its bias, specifically its tendency to over-predict interactions. Like above, we can do so by defining a bias matrix <span class="math inline"><strong>B</strong></span>, where interactions are over-predicted with probability <span class="math inline"><em>b</em></span>, and express the final classifier confusion matrix as <span class="math inline"><strong>M</strong> ⊙ <strong>S</strong> ⊙ <strong>B</strong></span>, <em>i.e.</em></p>
            <p><span class="math display">$$
            \begin{pmatrix}
                \rho^2 &amp; \rho (1-\rho) \\
                (1-\rho) \rho &amp; (1-\rho)^2
            \end{pmatrix} \odot \begin{pmatrix}
                s &amp; (1-s) \\
                (1-s) &amp; s
            \end{pmatrix} \odot \begin{pmatrix}
                b &amp; b \\
                (1-b) &amp; (1-b)
            \end{pmatrix}\,.
            $$</span></p>
            <p>The final expression for the confusion matrix in which we can regulate the skill and the bias is</p>
            <p><span class="math display">$$
            \mathbf{C} = \begin{pmatrix}
                s\times b\times \rho^2 &amp; (1-s)\times b\times \rho (1-\rho) \\
                (1-s)\times (1-b)\times (1-\rho) \rho &amp; s\times (1-b)\times (1-\rho)^2
            \end{pmatrix} \,.
            $$</span></p>
            <h2 id="what-are-the-baseline-values-of-performance-measures">What are the baseline values of performance measures?</h2>
            <p>In this section, we will change the values of <span class="math inline"><em>b</em></span> abd <span class="math inline"><em>s</em></span> for a given value of <span class="math inline"><em>ρ</em></span>, and see how the values of common performance measures for binary classification are affected. Specifically, we will focus on four quantities: the accuracy (<span class="math inline">(tp + tn)/(tp + tn + fp + fn)</span>), the <em>balanced</em> accuracy (<span class="math inline">tp/(2(tp + fn)) + tn/(2(tn + fp))</span>), Youden’s J (<span class="math inline">tp/(tp + fn) + tn/(tn + fp) − 1</span>), and the <span class="math inline"><em>F</em><sub>1</sub></span> score (<span class="math inline">2tp/(2tp + fp + fn)</span>).</p>
            <p><strong>Justification</strong> of why these 4</p>
            <p>Assuming a no-skill unbiased classifier (<em>i.e.</em> <span class="math inline"><strong>C</strong> = <strong>M</strong></span>), the accuracy is <span class="math inline"><em>ρ</em><sup>2</sup> + (1 − <em>ρ</em>)<sup>2</sup></span>, the balanced accuracy is <span class="math inline">0.5</span>, Youden’s J is <span class="math inline">0</span>, and <span class="math inline"><em>F</em><sub>1</sub> = <em>ρ</em></span>. In other words, given a connectance <span class="math inline"><em>ρ</em> = 0.05</span>, we expect that a classifier guessing at random would still achieve an accuracy of <span class="math inline">0.905</span>; for a connectance of <span class="math inline"><em>ρ</em> = 0.01</span>, this accuracy <em>increases</em> to over <span class="math inline">0.98</span>. In other words, networks with fewer interactions have inherently higher accuracy, because it is easy to predict the overwheling majority of non-interactions right.</p>
            <p>In order to examine how these values change w.r.t. the skill and bias, we performed a grid exploration of the values of <span class="math inline"><em>s</em></span> (from 0 to 1), and of <span class="math inline">logit(<em>b</em>)</span>, and visualize the result for a connectance of <strong>TODO</strong>.</p>
            <h1 id="numerical-experiments">Numerical experiments</h1>
            <p>In the following section, we will generate random networks, and train four binary classifiers (as well as an ensemble model using the sum of the outputs) on 30% of the interaction data. Networks are generated by picking random generality <span class="math inline"><em>g</em></span> and vulnerability <span class="math inline"><em>v</em></span> traits for <span class="math inline"><em>S</em> = 200</span> species uniformly on the unit interval, and assigning an interaction from species <span class="math inline"><em>i</em></span> to species <span class="math inline"><em>j</em></span> if <span class="math inline">0.2<em>g</em><sub><em>i</em></sub> − <em>ξ</em> ≤ <em>v</em><sub><em>j</em></sub> ≤ 0.2<em>g</em><sub><em>i</em></sub> + <em>ξ</em></span>, where <span class="math inline"><em>ξ</em></span> is a constant regulating the connectance of the networks, and varies uniformly in <span class="math inline">[5 × 10<sup> − 3</sup>, 10<sup> − 1</sup>]</span>. This model gives fully interval networks that are close analogues to the niche model <span class="citation" data-cites="Williams2000SimRul">(Williams &amp; Martinez, 2000)</span>, but has the benefit of only relying on two features (<span class="math inline"><em>g</em><sub><em>i</em></sub>, <em>v</em><sub><em>j</em></sub></span>), and having the exact same rule for all interactions. It is, therefore, a simple case which most classifiers should be able to learn.</p>
            <p>The training sample is composed of 30% of the <span class="math inline">4 × 10<sup>4</sup></span> possible entries in the network, <em>i.e.</em> <span class="math inline"><em>n</em> = 12000</span>. Out of these interactions, we pick a proportion <span class="math inline"><em>ν</em></span> (the training set bias) to be positive, so that the training set has <span class="math inline"><em>ν</em><em>n</em></span> interactions, and <span class="math inline">(1 − <em>ν</em>)<em>n</em></span> non-interactions. We vary <span class="math inline"><em>ν</em></span> uniformly in <span class="math inline">]0, 1[</span>. This allows to evaluate how the measures of binary classification performance respond to artificially rebalanced dataset for a given network connectance. Note that both <span class="math inline"><em>ξ</em></span> and <span class="math inline"><em>ν</em></span> are sampled from a distribution rather than being picked on a grid; this is because there is no direct relationship between the value of <span class="math inline"><em>ξ</em></span> and the connectance of the simulated network, and therefore the precise value of <span class="math inline"><em>ξ</em></span> is not relevant for the analysis of the results.</p>
            <p>The dataset used for numerical experiments is composed of 20000 such <span class="math inline">(<em>ξ</em>, <em>ν</em>)</span> pairs, on which four learners are trained: a decision tree regressor, a boosted regression tree, a ridge regressor, and a random forest regressor. All models were taken from the <code>MLJ.jl</code> package <span class="citation" data-cites="Blaom2020MljJul Blaom2020FleMod">(Blaom et al., 2020; Blaom &amp; Vollmer, 2020)</span> in Julia 1.7 <span class="citation" data-cites="Bezanson2017JulFre">(Bezanson et al., 2017)</span>. In order to pick the best adjacency matrix for a given learner, we performed a thresholding approach using 500 steps on predictions from the testing set, and picking the threshold that maximized Youden’s informedness, which is usually the optimized target for imbalanced classification. During the thresholding step, we measured the area under the receiving-operator characteristic (ROC-AUC) and precision-recall (PR-AUC) curves, as measures of overall performance over the range of returned values. We report the ROC-AUC and PR-AUC, as well as a suite of other measures as introduced in the next section, for the best threshold. The ensemble model was generated by summing the predictions of all component models on the testing set (ranged in <span class="math inline">[0, 1]</span>), then put through the same thresholding process. The complete code to run the simulations is given as an appendix.</p>
            <p>After the simulations were completed, we removed all runs (<em>i.e.</em> pairs of <span class="math inline"><em>ξ</em></span> and <span class="math inline"><em>ν</em></span>) for which at least one of the following conditions was met: the accuracy was 0, the true positive or true negative rates were 0, the connectance was larger than 0.2. This removes both the obviously failed model runs, and the networks that are more densely connected compared to the connectance of empirical food webs (and are therefore less difficult to predict, being less imbalanced).</p>
            <h2 id="effect-of-training-set-bias-on-performance">Effect of training set bias on performance</h2>
            <h2 id="required-amount-of-positives-to-get-the-best-performance">Required amount of positives to get the best performance</h2>
            <h1 id="guidelines-for-prediction">Guidelines for prediction</h1>
            <h1 class="unnumbered" id="references">References</h1>
            <div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="doc-bibliography">
            <div id="ref-Allouche2006AssAcc" class="csl-entry" role="doc-biblioentry">
            Allouche, O., Tsoar, A., &amp; Kadmon, R. (2006). Assessing the accuracy of species distribution models: Prevalence, kappa and the true skill statistic (TSS). <em>Journal of Applied Ecology</em>, <em>43</em>(6), 1223–1232. <a href="https://doi.org/10.1111/j.1365-2664.2006.01214.x">https://doi.org/10.1111/j.1365-2664.2006.01214.x</a>
            </div>
            <div id="ref-Bezanson2017JulFre" class="csl-entry" role="doc-biblioentry">
            Bezanson, J., Edelman, A., Karpinski, S., &amp; Shah, V. (2017). Julia: A Fresh Approach to Numerical Computing. <em>SIAM Review</em>, <em>59</em>(1), 65–98. <a href="https://doi.org/10.1137/141000671">https://doi.org/10.1137/141000671</a>
            </div>
            <div id="ref-Blaom2020MljJul" class="csl-entry" role="doc-biblioentry">
            Blaom, A. D., Kiraly, F., Lienart, T., Simillides, Y., Arenas, D., &amp; Vollmer, S. J. (2020). MLJ: A Julia package for composable machine learning. <em>Journal of Open Source Software</em>, <em>5</em>(55), 2704. <a href="https://doi.org/10.21105/joss.02704">https://doi.org/10.21105/joss.02704</a>
            </div>
            <div id="ref-Blaom2020FleMod" class="csl-entry" role="doc-biblioentry">
            Blaom, A. D., &amp; Vollmer, S. J. (2020, December 31). <em>Flexible model composition in machine learning and its implementation in MLJ</em>. <a href="http://arxiv.org/abs/2012.15505">http://arxiv.org/abs/2012.15505</a>
            </div>
            <div id="ref-Strydom2021RoaPre" class="csl-entry" role="doc-biblioentry">
            Strydom, T., Catchen, M. D., Banville, F., Caron, D., Dansereau, G., Desjardins-Proulx, P., Forero-Muñoz, N. R., Higino, G., Mercier, B., Gonzalez, A., Gravel, D., Pollock, L., &amp; Poisot, T. (2021). A roadmap towards predicting species interaction networks (across space and time). <em>Philosophical Transactions of the Royal Society B: Biological Sciences</em>, <em>376</em>(1837), 20210063. <a href="https://doi.org/10.1098/rstb.2021.0063">https://doi.org/10.1098/rstb.2021.0063</a>
            </div>
            <div id="ref-Whalen2021NavPit" class="csl-entry" role="doc-biblioentry">
            Whalen, S., Schreiber, J., Noble, W. S., &amp; Pollard, K. S. (2021). Navigating the pitfalls of applying machine learning in genomics. <em>Nature Reviews Genetics</em>, 1–13. <a href="https://doi.org/10.1038/s41576-021-00434-9">https://doi.org/10.1038/s41576-021-00434-9</a>
            </div>
            <div id="ref-Williams2000SimRul" class="csl-entry" role="doc-biblioentry">
            Williams, R., &amp; Martinez, N. (2000). Simple rules yield complex food webs. <em>Nature</em>, <em>404</em>, 180–183. <a href="http://userwww.sfsu.edu/ ">http://userwww.sfsu.edu/</a>
            </div>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <h2>Affiliations</h2> 
            <sup>1</sup>&nbsp;Université de Montréal; <sup>2</sup>&nbsp;Québec Centre for Biodiversity Sciences
            <br />
            
        </div>
    </footer>
    
</body>
</html>