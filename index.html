<!DOCTYPE html>
<html lang="" dir="ltr">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Guidelines for the validation of machine learning predictions of species interactions</title>
    <script src="https://hypothes.is/embed.js" async></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css"
    integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,300;0,400;0,500;1,300;1,400;1,500&display=swap');
        @import url('https://fonts.googleapis.com/css2?family=Roboto+Condensed:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap');
        @import url('https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,400;0,500;1,400;1,500&display=swap');

        :root {
            --citation: #1976D2;
            --structure: hsl(210, 61%, 43%);
            
            --light: #474747;
            --pale: #a5a5a5;
            --grey: #e0e0e0;
            --nav: #fdfdfd;

            --width: 820px;
            --inner-space: 1.2em;
        }

        body {
            font-family: "Roboto", sans-serif;
            margin: 0 0;
            padding: 0 0;
        }

        article > p {
            line-height: 135%;
        }
        article > h1 {
            padding-top: 3em;
        }
        a {
            text-decoration: none;
            color: #000;
            border-bottom: 1px solid var(--structure);
        }
        a:hover {
            font-weight: 500;
        }
        .citation {
            color: var(--citation);
        }
        .container {
            margin: 0em auto;
            max-width: var(--width);
            padding: var(--inner-space);
        }
        header > .container {
            margin-top: 4em;
        }
        code, pre {
            font-family: "Roboto Mono", monospace;
        }
        pre {
            white-space: pre-wrap;
        }
        h1, h2, h3 {
            color: var(--structure);
            font-family: "Roboto Condensed", sans-serif;
            font-weight: 400;
        }
        header {
            font-family: "Roboto Condensed", sans-serif;
            z-index: 5;
            color: black;
            margin-top: 4rem;
        }
        .header--title {
            font-weight: 400;
            font-size: 2rem;
            margin-bottom: 1rem;
        }
        .header--authors > span {
            color: var(--light);
            font-weight: 400;
        }
        main {
            padding-top: 8em !important;
            padding-bottom: 8em !important;
        }
        footer {
            background-color: var(--grey);
            font-weight: 300;
        }
        .abstract {
            color: var(--light);
            margin-bottom: 5em;
        }
        figure {
            max-width: 100%;
            border: 1px solid var(--pale);
            border-radius: 2px;
            margin: 1em 0.5em;
        }
        figure > img {
            max-width: 100%;
            display: block;
            margin: 0em auto;
        }
        figure > figcaption {
            padding: var(--inner-space);
        }
        table {
            width: 100%;
            min-width: 100%;
            border-collapse: collapse;
        }
        table > caption {
            padding: var(--inner-space);
        }
        figcaption, caption {
            color: var(--light);
            font-weight: 300;
        }
        thead {
            color: var(--structure);
        }
        td, th {
            padding: 3px;
        }
        tr {
            border-top: 1px solid var(--structure);
            border-bottom: 1px solid var(--structure);
        }
        tbody > tr:nth-child(even) {background: var(--grey)};
        tbody {
            border-bottom: 1px solid var(--structure);
        }
        #refs {
            font-size: 90%;
            column-count: 2;
        }
        #refs > .csl-entry {
            padding-bottom: 0.2em;
            padding-top: 0.2em;
        }
        .hanging-indent > .csl-entry {
            padding-left: 15px ;
            text-indent: -15px ;
        }    </style>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css"
    integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous" />
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js"
    integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O"
    crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js"
    integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF"
    crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false }
                ]
            });
        });
    </script>
</head>

<body>
    <header>
        <div class="container">
            <div class="header--title">Guidelines for the validation of machine learning predictions of species interactions</div>
            <div class="header--authors">
                <span class="header--author">
                <a href="https://orcid.org/0000-0002-0735-5184" target="_blank">
                    Timoth√©e Poisot
                </a>
                <sup class="affilcode">1</sup>
                <sup class="affilcode">2</sup>
                </span>
            </div>
        </div>
    </header>
    
    <main class="container">
        <section class="abstract">
            
            <ol>
                <li>The prediction of species interactions is gaining momentum as a way to circumvent limitations in data volume. Yet, ecological networks are challenging to predict because they are typically small and sparse. Dealing with extreme class imbalance is a challenge for most binary classifiers, and there are currently no guidelines as to how predictive models can be trained for this specific problem.</li>
                <li>Using simple mathematical arguments and numerical experiments in which a variety of classifiers (for supervised learning) are trained on simulated networks, we develop a series of guidelines related to the choice of measures to use for model selection, and the degree of unbiasing to apply to the training dataset.</li>
                <li>Neither classifier accuracy nor the ROC-AUC are informative measures for the performance of interaction prediction. PR-AUC is a fairer assessment of performance. In some cases, even standard measures can lead to selecting a more biased classifier because the effect of connectance is strong. The amount of correction to apply to the training dataset depends on network connectance, on the measure to be optimized, and only weakly on the classifier.</li>
                <li>These results reveal that training machines to predict networks is a challenging task, and that in virtually all cases, the composition of the training set needs to be experimented on before performing the actual training. We discuss these consequences in the context of the low volume of data.</li>
            </ol>

            <div class="artifacts">
                <a href="draft.pdf" title="Draft PDF" target="_blank">draft</a>
                <a href="preprint.pdf" title="Preprint PDF" target="_blank">preprint</a>
            </div>
        </section>
        
        <article>
            <p>The accuracy paradox is the basis of a number of problems in statistical education, and lies in the fact that, when the desired class is rare, a model that gets less and less performant will become more and more accurate and useful, simply by (i) underpredicting true positive cases and (ii) over-predicting false negatives. In other words, accuracy, defined as the proportion of predictions that are correct, is often useless as a measure of how predictive a model is. This is particularly true in ecological networks; the desired class (presence of an interaction between two species) is the one we care most about, and by far the least commmon. Herein lies the core challenge of predicting species interactions: the extreme imbalance between classes makes the training of predictive models difficult, and their validation even more so as we do not reliably know which negatives are true. The connectance of empirical networks is usually well under 20%, with larger networks having a lower connectance <span class="citation" data-cites="MacDonald2020RevLin">(MacDonald et al., 2020)</span>. Recent contributions <span class="citation" data-cites="Strydom2021RoaPre Becker2021OptPre">(Becker et al., 2021; Strydom et al., 2021)</span> highlight that predictive models of interactions can likely be improved by adding information (in the form of, <em>e.g.</em> traits), but that we do not have robust guidelines as to how the predictive ability of these models should be evaluated, nor about how the models should be trained. Here, by relying on simple derivations and a series of simulations, we formulate a number of such guidelines, specifically for the case of binary classifiers derived from thresholded values.</p>
            <p>Binary classifiers are usually assessed by measuring properties of their confusion matrix, <em>i.e.</em> the contingency table reporting true/false positive/negative hits. A confusion matrix is laid out as</p>
            <p><span class="math display">$$\begin{pmatrix}
                \text{tp} &amp; \text{fp} \\
                \text{fn} &amp; \text{tn}
            \end{pmatrix} \,,$$</span></p>
            <p>wherein <span class="math inline">tp</span> is the number of interactions predicted as positive, <span class="math inline">tn</span> is the number of non-interactions predicted as negative, <span class="math inline">fp</span> is the number of non-interactions predicted as positive, and <span class="math inline">fn</span> is the number of interactions predicted as negative. Almost all measures based on the confusion matrix express rates of error or success as proportions, and therefore the values of these components matter in a <em>relative</em> way. At a coarse scale, a classifier is <em>accurate</em> when the trace of the matrix divided by the sum of the matrix is close to 1, with other measures focusing on different ways in which the classifer is wrong.</p>
            <p>There is an immense diversity of measures to evaluate the performance of classification tasks <span class="citation" data-cites="Ferri2009ExpCom">(Ferri et al., 2009)</span>. Here we will focus on five of them with high relevance for imbalanced learning <span class="citation" data-cites="He2013ImbLea">(He &amp; Ma, 2013)</span>; three threshold metrics (<span class="math inline"><em>Œ∫</em></span>, informedness, and MMC, the Matthews Correlation Coefficient), and two ranking metrics (the areas under the Receiving Operator Characteristic and the Precision-Recall curves; resp. ROC-ACU and PR-AUC). The <span class="math inline"><em>Œ∫</em></span> measure of agreement <span class="citation" data-cites="Landis1977MeaObs">(Landis &amp; Koch, 1977)</span> establishes the extent to which two observers (here the data and the prediction) agree, and is measured as</p>
            <p><span class="math display">$$
            2\frac{tp\times tn - fn\times fp}{(tp+fp)\times (fp+tn)+(tn+fp)\times (tn+fn)} \,.
            $$</span></p>
            <p>Informedness <span class="citation" data-cites="Youden1950IndRat">(Youden, 1950)</span> (also known as bookmaker informedness or the True Skill Statistic) is <span class="math inline">TPR‚ÄÖ+‚ÄÖTNR‚ÄÖ‚àí‚ÄÖ1</span>, where <span class="math inline">TPR‚ÄÑ=‚ÄÑ<em>t</em><em>p</em>/(<em>t</em><em>p</em>‚ÄÖ+‚ÄÖ<em>f</em><em>n</em>)</span> and <span class="math inline">TNR‚ÄÑ=‚ÄÑ<em>t</em><em>n</em>/(<em>t</em><em>n</em>‚ÄÖ+‚ÄÖ<em>f</em><em>p</em>)</span>; informedness can be used to find the optimal cutpoint in thresholding analyses <span class="citation" data-cites="Schisterman2005OptCut">(Schisterman et al., 2005)</span>. The MCC is defined as</p>
            <p><span class="math display">$$
            \frac{tp\times tn - fn\times fp}{\sqrt{(tp+fp)\times (tp+fn)\times (tn+fp)\times (tn+fn)}} \,.
            $$</span></p>
            <p>Finally, <span class="math inline"><em>F</em><sub>1</sub></span> is the harmonic mean of precision (the chance that a positive even was correctly classified) and sensitivity (the ability to correctly classify positive events), and is defined as</p>
            <p><span class="math display">$$
            2\frac{tp}{2\times tp + fp + fn}\,.
            $$</span></p>
            <p>A lot of binary classifiers are built by using a regressor (whose task is to guess the value of the interaction, and can therefore return a value considered to be a pseudo-probability); in this case, the optimal value below which predictions are assumed to be negative (<em>i.e.</em> the interaction does not exist) can be determined by picking a threshold maximizing some value on the ROC curve or the PR curve. The area under these curves (ROC-AUC and PR-AUC henceforth) give ideas on the overall goodness of the classifier. <span class="citation" data-cites="Saito2015PrePlo">Saito &amp; Rehmsmeier (2015)</span> established that the ROC-AUC is biased towards over-estimating performance for imbalanced data; on the contrary, the PR-AUC is able to identify classifiers that are less able to detect positive interactions correctly, with the additional advantage of having a baseline value equal to prevalence. Therefore, it is important to assess whether these two measures return different results when applied to ecological network prediction. The ROC curve is defined by the false positive rate on the <span class="math inline"><em>x</em></span> axis, and the true positive rate on the <span class="math inline"><em>y</em></span> axis, and the PR curve is defined by the true positive rate on the <span class="math inline"><em>x</em></span> axis, and the positive predictive value on the <span class="math inline"><em>y</em></span> axis. By comparison with the previous paragraph, it is obvious that <span class="math inline"><em>F</em><sub>1</sub></span> has ties to the PR curve (being close to the expected PR-AUC), and that informedness has ties to the ROC curve (whereby the threshold maximizing informedness is also the point of maximal inflection on the ROC curve). One important difference between ROC and PR is that the later does not prominently account for the size of the true negative compartments: in short, it is more sensitive to the correct positive predictions. In a context of strong imbalance, PR-AUC is therefore a more stringent test of model performance.</p>
            <p>The same approach is used to evaluate <em>e.g.</em> species distribution models (SDMs). Indeed, the training and evaluation of SDMs as binary classifiers suffers from the same issue of low prevalence; this is not surprising that the two fields (SDMs and network predictions) would share methods and their attached conceptual issues, as they suffer from data limitations, class imbalance, and the conversion of quantitative prediction into a binary classification. In previous work, <span class="citation" data-cites="Allouche2006AssAcc">Allouche et al. (2006)</span> suggested that <span class="math inline"><em>Œ∫</em></span> was a better test of model performance than the True Skill Statistic (TSS; which we refer to as Youden‚Äôs informedness); these conclusions were later criticized by <span class="citation" data-cites="Somodi2017PreDep">Somodi et al. (2017)</span>, who emphasized that informedness‚Äô relationship to prevalence depends on assumptions about bias in the model, and therefore recommend the use of <span class="math inline"><em>Œ∫</em></span> as a validation of classification performance. Although this work offers recommendations about the comparison of models, it doesn‚Äôt establishes baselines or good practices for training on imbalanced ecological data. Within the context of networks, there are three specific issues that need to be adressed. First, what values of performance measures are we expecting for a classifier that has poor performance? This is particularly important as it can evaluate whether low prevalence can lull us into a false sense of predictive accuracy. Second, independently of the question of model evaluation, is low prevalence an issue for <em>training</em>, and can we remedy it? Finally, because the low amount of data on interaction makes a lot of imbalance correction methods <span class="citation" data-cites="Branco2015SurPre">(see <em>e.g.</em> Branco et al., 2015)</span> hard to apply, which indicators can be optimized with the least amount of positive interaction data?</p>
            <p>In addition to the literature on SDMs, most of the research on machine learning application to life sciences is focused on genomics <span class="citation" data-cites="Whalen2021NavPit">(which has very specific challenges, see a recent discussion by Whalen et al., 2021)</span>; this sub-field has generated largely different recommendations. <span class="citation" data-cites="Chicco2020AdvMat">Chicco &amp; Jurman (2020)</span> suggest using Matthews correlation coefficient (MCC) over <span class="math inline"><em>F</em><sub>1</sub></span>, as a protection against over-inflation of predicted results; <span class="citation" data-cites="Delgado2019WhyCoh">Delgado &amp; Tibau (2019)</span> advocate against the use of Cohen‚Äôs <span class="math inline"><em>Œ∫</em></span>, again in favor of MCC, as the relative nature of <span class="math inline"><em>Œ∫</em></span> means that a worse classifier can be picked over a better one; similarly, <span class="citation" data-cites="Boughorbel2017OptCla">Boughorbel et al. (2017)</span> recommend MCC over other measures of performance for imbalanced data, as it has more desirable statistical properties. More recently, <span class="citation" data-cites="Chicco2021MatCor">Chicco et al. (2021)</span> temper the apparent supremacy of the MCC, by suggesting it should be replaced by Youden‚Äôs informedness (also known as <span class="math inline"><em>J</em></span>, bookmaker‚Äôs accuracy, and the True-Skill Statistic) when the imbalance in the dataset may not be representative <span class="citation" data-cites="Jordano2016SamNet Jordano2016ChaEco">(Jordano, 2016a, which is the case as networks are under-sampled; 2016b)</span>, when classifiers need to be compared across different datasets <span class="citation" data-cites="McLeod2021SamAsy">(for example when predicting a system in space, where undersampling varies locally; McLeod et al., 2021)</span>, and when comparing the results to a no-skill (baseline) classifier is important. As these conditions are likely to be met with network data, there is a need to evaluate which measures of classification accuracy respond in a desirable way.</p>
            <p>We establish that due to the low prevalence of interactions, even poor classifiers applied to food web data will reach a high accuracy; this is because the measure is dominated by the accidentally correct predictions of negatives. On simulated confusion matrices with ranges of imbalance that are credible for ecological networks, MCC had the most desirable behavior, and informedness is a linear measure of classifier skill. By performing simulations with four models and an ensemble, we show that informedness and ROC-AUC are consistently high on network data, and that MCC and PR-AUC are more accurate measures of the effective performance of the classifier. Finally, by measuring the structure of predicted networks, we highlight an interesting paradox: the models with the best performance measures are not the models with the closest reconstructed network structure. We discuss these results in the context of establishing guidelines for the prediction of ecological interactions.</p>
            <h1 id="baseline-values">Baseline values</h1>
            <p>In this section, we will assume a network of connectance <span class="math inline"><em>œÅ</em></span>, <em>i.e.</em> having <span class="math inline"><em>œÅ</em><em>S</em><sup>2</sup></span> interactions (where <span class="math inline"><em>S</em></span> is the species richness), and <span class="math inline">(1‚ÄÖ‚àí‚ÄÖ<em>œÅ</em>)<em>S</em><sup>2</sup></span> non-interactions. Therefore, the vector describing the <em>true</em> state of the network (assumed to be an unweighted, directed network) is a column vector <span class="math inline"><strong>o</strong><sup><em>T</em></sup>‚ÄÑ=‚ÄÑ[<em>œÅ</em>(1‚ÄÖ‚àí‚ÄÖ<em>œÅ</em>)]</span> (we can safely drop the <span class="math inline"><em>S</em><sup>2</sup></span> terms, as we will work on the confusion matrix, which ends up expressing <em>relative</em> values). We will apply skill and bias to this matrix, and measure how a selection of performance metrics respond to changes in these values, in order to assess their suitability for model evaluation.</p>
            <h2 id="confusion-matrix-with-skill-and-bias">Confusion matrix with skill and bias</h2>
            <p>In order to write the values of the confusion matrix for a hypothetical classifier, we need to define two characteristics: its skill, and its bias. Skill, here, refers to the propensity of the classifier to get the correct answer (<em>i.e.</em> to assign interactions where they are, and to not assign them where they are not). A no-skill classifier guesses at random, <em>i.e.</em> it will guess interactions with a probability <span class="math inline"><em>œÅ</em></span>. The predictions of a no-skill classifier can be expressed as a row vector <span class="math inline"><strong>p</strong>‚ÄÑ=‚ÄÑ[<em>œÅ</em>(1‚ÄÖ‚àí‚ÄÖ<em>œÅ</em>)]</span>. The confusion matrix <span class="math inline"><strong>M</strong></span> for a no-skill classifier is given by the element-wise product of these vectors <span class="math inline"><strong>o</strong>‚ÄÖ‚äô‚ÄÖ<strong>p</strong></span>, <em>i.e.</em></p>
            <p><span class="math display">$$
            \mathbf{M} = \begin{pmatrix}
                \rho^2 &amp; \rho (1-\rho) \\
                (1-\rho) \rho &amp; (1-\rho)^2
            \end{pmatrix} \,.
            $$</span></p>
            <p>In order to regulate the skill of this classifier, we can define a skill matrix <span class="math inline"><strong>S</strong></span> with diagonal elements equal to <span class="math inline"><em>s</em></span>, and off-diagonal elements equal to <span class="math inline">(1‚ÄÖ‚àí‚ÄÖ<em>s</em>)</span>, and re-express the skill-adjusted confusion matrix as <span class="math inline"><strong>M</strong>‚ÄÖ‚äô‚ÄÖ<strong>S</strong></span>, <em>i.e.</em></p>
            <p><span class="math display">$$
            \begin{pmatrix}
                \rho^2 &amp; \rho (1-\rho) \\
                (1-\rho) \rho &amp; (1-\rho)^2
            \end{pmatrix} \odot \begin{pmatrix}
                s &amp; (1-s) \\
                (1-s) &amp; s
            \end{pmatrix} \,.
            $$</span></p>
            <p>Note that when <span class="math inline"><em>s</em>‚ÄÑ=‚ÄÑ0</span>, <span class="math inline">Tr(<strong>M</strong>)‚ÄÑ=‚ÄÑ0</span> (the classifier is <em>always</em> wrong), when <span class="math inline"><em>s</em>‚ÄÑ=‚ÄÑ0.5</span>, the classifier is no-skill and guesses at random, and when <span class="math inline"><em>s</em>‚ÄÑ=‚ÄÑ1</span>, the classifier is perfect.</p>
            <p>The second element we can adjust in this hypothetical classifier is its bias, specifically its tendency to over-predict interactions. Like above, we can do so by defining a bias matrix <span class="math inline"><strong>B</strong></span>, where interactions are over-predicted with probability <span class="math inline"><em>b</em></span>, and express the final classifier confusion matrix as <span class="math inline"><strong>M</strong>‚ÄÖ‚äô‚ÄÖ<strong>S</strong>‚ÄÖ‚äô‚ÄÖ<strong>B</strong></span>, <em>i.e.</em></p>
            <p><span class="math display">$$
            \begin{pmatrix}
                \rho^2 &amp; \rho (1-\rho) \\
                (1-\rho) \rho &amp; (1-\rho)^2
            \end{pmatrix} \odot \begin{pmatrix}
                s &amp; (1-s) \\
                (1-s) &amp; s
            \end{pmatrix} \odot \begin{pmatrix}
                b &amp; b \\
                (1-b) &amp; (1-b)
            \end{pmatrix}\,.
            $$</span></p>
            <p>The final expression for the confusion matrix in which we can regulate the skill and the bias is</p>
            <p><span class="math display">$$
            \mathbf{C} = \begin{pmatrix}
                s\times b\times \rho^2 &amp; (1-s)\times b\times \rho (1-\rho) \\
                (1-s)\times (1-b)\times (1-\rho) \rho &amp; s\times (1-b)\times (1-\rho)^2
            \end{pmatrix} \,.
            $$</span></p>
            <p>In all further simulations, the confusion matrix <span class="math inline"><strong>C</strong></span> is transformed so that it sums to 1.</p>
            <h2 id="what-are-the-baseline-values-of-performance-measures">What are the baseline values of performance measures?</h2>
            <p>In this section, we will change the values of <span class="math inline"><em>b</em></span>, <span class="math inline"><em>s</em></span>, and <span class="math inline"><em>œÅ</em></span>, and report how the main measures discussed in the introduction (MCC, <span class="math inline"><em>F</em><sub>1</sub></span>, <span class="math inline"><em>Œ∫</em></span>, and informedness) are responding to issues with the classifier. Before we do so, it is important to explain why we will not focus on accuracy too much. Accuracy is the number of correct predictions (<span class="math inline">Tr(<strong>C</strong>)</span>) divided by the sum of the confusion matrix. For a no-skill, no-bias classifier, accuracy is equal to <span class="math inline"><em>œÅ</em><sup>2</sup>‚ÄÖ+‚ÄÖ(1‚ÄÖ‚àí‚ÄÖ<em>œÅ</em>)<sup>2</sup></span>; for <span class="math inline"><em>œÅ</em>‚ÄÑ=‚ÄÑ0.05</span>, this is <span class="math inline">‚ÄÑ‚âà‚ÄÑ0.90</span>, and for <span class="math inline"><em>œÅ</em>‚ÄÑ=‚ÄÑ0.01</span>, this is equal to <span class="math inline">‚ÄÑ‚âà‚ÄÑ0.98</span>. In other words, the values of accuracy are expected to be so high that they are not really informatived (this is simply explained by the fact that for <span class="math inline"><em>œÅ</em></span> small, <span class="math inline"><em>œÅ</em><sup>2</sup>‚ÄÑ‚â™‚ÄÑ(1‚ÄÖ‚àí‚ÄÖ<em>œÅ</em>)<sup>2</sup></span>). More concerning is the fact that introducing bias changes the response of accuracy in unexpected ways. Assuming a no-skill classifier, the numerator of accuracy becomes <span class="math inline"><em>b</em><em>œÅ</em><sup>2</sup>‚ÄÖ+‚ÄÖ(1‚ÄÖ‚àí‚ÄÖ<em>b</em>)(1‚ÄÖ‚àí‚ÄÖ<em>œÅ</em>)<sup>2</sup></span>, which increases when <span class="math inline"><em>b</em></span> is low, which specifically means that at equal skill, a classifier that under-predicts interactions will have higher accuracy than an un-biased classifier. These issues are absent from balanced accuracy, but should nevertheless lead us to not report accuracy as the primary measure of network prediction success; moving forward, we will focus on other measures.</p>
            <p>In order to examine how MCC, <span class="math inline"><em>F</em><sub>1</sub></span>, <span class="math inline"><em>Œ∫</em></span>, and informedness change w.r.t. the imbalance, skill, and bias, we performed a grid exploration of the values of <span class="math inline">logit(<em>s</em>)</span> and <span class="math inline">logit(<em>b</em>)</span> linearly from <span class="math inline">‚ÄÖ‚àí‚ÄÖ10</span> to <span class="math inline">10</span>, of <span class="math inline"><em>œÅ</em></span> linearly in <span class="math inline">]0,‚ÄÜ0.5]</span>, which is within the range of usually observed connectance values for empirical food webs. Note that at this point, there is no food web model to speak of; rather, the confusion matrix we discuss can be obtained for any classification task. Based on the previous discussion, the desirable properties for a measure of classifier success should be: an increase with classifier skill, especially at low bias; a hump-shaped response to bias, especially at high skill, and ideally center around <span class="math inline">logit(<em>b</em>)‚ÄÑ=‚ÄÑ0</span>; an increase with prevalence up until equiprevalence is reached.</p>
            <figure>
            <img src="figures/changing-bias.png" id="fig:bias" alt="Figure 1: Consequences of changing the classifier skills (s) and bias (s) for a connectance \rho=0.15, on accuracy, F_1, postive predictive value, and \kappa. Accuracy increases with skill, but also increases when the bias tends towards estimating fewer interactions. The F_1 score increases with skill but also increases when the bias tends towards estimating more interactions; PPV behaves in the same way. Interestingly, \kappa responds as expected to skill (being negative whenever s &lt; 0.5), and peaks for values of b \approx 0.5; nevertheless, the value of bias for which \kappa is maximized in not b=0.5, but instead increases with classifier skill. In other words, at equal skill, maximizing \kappa would lead to select a more biased classifier." /><figcaption aria-hidden="true">Figure 1: Consequences of changing the classifier skills (<span class="math inline"><em>s</em></span>) and bias (<span class="math inline"><em>s</em></span>) for a connectance <span class="math inline"><em>œÅ</em>‚ÄÑ=‚ÄÑ0.15</span>, on accuracy, <span class="math inline"><em>F</em><sub>1</sub></span>, postive predictive value, and <span class="math inline"><em>Œ∫</em></span>. Accuracy increases with skill, but also increases when the bias tends towards estimating <em>fewer</em> interactions. The <span class="math inline"><em>F</em><sub>1</sub></span> score increases with skill but also increases when the bias tends towards estimating <em>more</em> interactions; PPV behaves in the same way. Interestingly, <span class="math inline"><em>Œ∫</em></span> responds as expected to skill (being negative whenever <span class="math inline"><em>s</em>‚ÄÑ&lt;‚ÄÑ0.5</span>), and peaks for values of <span class="math inline"><em>b</em>‚ÄÑ‚âà‚ÄÑ0.5</span>; nevertheless, the value of bias for which <span class="math inline"><em>Œ∫</em></span> is maximized in <em>not</em> <span class="math inline"><em>b</em>‚ÄÑ=‚ÄÑ0.5</span>, but instead increases with classifier skill. In other words, at equal skill, maximizing <span class="math inline"><em>Œ∫</em></span> would lead to select a <em>more</em> biased classifier.</figcaption>
            </figure>
            <p>In fig.¬†1, we show that none of the four measures satisfy all the considerations at once: <span class="math inline"><em>F</em><sub>1</sub></span> increases with skill, and increases monotonously with bias; this is because <span class="math inline"><em>F</em><sub>1</sub></span> does not account for true negatives, and the increase in positive detection masks the over-prediction of interactions. Informedness varies with skill, reaching 0 for a no-skill classifier, but is entirely unsensitive to bias. Both MCC and <span class="math inline"><em>Œ∫</em></span> have the same behavior, whereby they increase with skill. <span class="math inline"><em>Œ∫</em></span> peaks at increasing values of biass for increasing skill, <em>i.e.</em> is likely to lead to the selection of a classifier that over-predicts interactions. By contract, MCC peaks at the same value, regardless of skill, but this value is not <span class="math inline">logit(<em>b</em>)‚ÄÑ=‚ÄÑ0</span>: unless at very high classifier skill, MCC risks leading to a model that over-predicts interactions. In fig.¬†2, we show that all measures except <span class="math inline"><em>F</em><sub>1</sub></span> give a value of 0 for a no-skill classifier, and are forced towars their correct maximal value when skill changes (<em>i.e.</em> a more connected networks will have higher values for a skilled classifierd, and lower values for a classifier making mostly mistakes).</p>
            <figure>
            <img src="figures/changing-connectance.png" id="fig:connectance" alt="Figure 2: As in fig.¬†1, consequences of changing connectance for different levels of classifier skill, assuming no classifier bias. Informedness, \kappa, and MCC do increase with connectance, but only when the classifier is not no-skill; by way of contrast, a more connected network will give a higher F_1 value even with a no-skill classifier." /><figcaption aria-hidden="true">Figure 2: As in fig.¬†1, consequences of changing connectance for different levels of classifier skill, assuming no classifier bias. Informedness, <span class="math inline"><em>Œ∫</em></span>, and MCC do increase with connectance, but only when the classifier is not no-skill; by way of contrast, a more connected network will give a higher <span class="math inline"><em>F</em><sub>1</sub></span> value even with a no-skill classifier.</figcaption>
            </figure>
            <p>These two analyses point to the following recommendations: MCC is indeed more appropriate than <span class="math inline"><em>Œ∫</em></span>, as although sensitive to bias, it is sensitive in a consistent way. Informedness is appropriate at discriminating between different skills, but confounded by bias. As both of these measures bring valuable information on the model behavior, we will retain them for future analyses. <span class="math inline"><em>F</em><sub>1</sub></span> is increasing with bias, and should not be prioritized to evalue the performance of the model. The discussion of sensitivity to bias should come with a domain-specific caveat: although it is likely that interactions documented in ecological networks are correct, a lot of non-interactions are simply unobserved; as predictive models are used for data-inflation (<em>i.e.</em> the prediction of new interactions), it is not necessarily a bad thing in practice to select models that predict more interactions than the original dataset, because the original dataset misses some interactions. Furthermore, the weight of positive interactions could be adjusted if some information about the extent of undersampling exists <span class="citation" data-cites="Branco2015SurPre">(<em>e.g.</em> Branco et al., 2015)</span>. In a recent large-scale imputation of interactions in the mammal-virus networks, <span class="citation" data-cites="Poisot2021ImpMam">Poisot et al. (2021)</span> for example estimated that 93% of interactions are yet to be documented.</p>
            <h1 id="numerical-experiments-on-training-strategy">Numerical experiments on training strategy</h1>
            <p>In the following section, we will generate random bipartite networks (this works without loss of generality on unipartite networks), and train four binary classifiers (as well as an ensemble model using the sum of ranged outputs from the component models) on 30% of the interaction data. Networks are generated by picking a random infectiousness trait <span class="math inline"><em>v</em><sub><em>i</em></sub></span> for 100 species (from a <span class="math inline"><em>B</em>(6,‚ÄÜ8)</span> distribution), and a resistance trait <span class="math inline"><em>h</em><sub><em>j</em></sub></span> for 100 species (from a <span class="math inline"><em>B</em>(2,‚ÄÜ8)</span> distribution). There is an interaction between <span class="math inline"><em>i</em></span> and <span class="math inline"><em>j</em></span> when <span class="math inline"><em>v</em><sub><em>i</em></sub>‚ÄÖ‚àí‚ÄÖ<em>Œæ</em>/2‚ÄÑ‚â§‚ÄÑ<em>h</em><sub><em>j</em></sub>‚ÄÑ‚â§‚ÄÑ<em>v</em><sub><em>i</em></sub>‚ÄÖ+‚ÄÖ<em>Œæ</em>/2</span>, where <span class="math inline"><em>Œæ</em></span> is a constant regulating the connectance of the network (there is an almost 1:1 relationship between <span class="math inline"><em>Œæ</em></span> and connectance), and varies uniformly in <span class="math inline">[0.05,‚ÄÜ0.35]</span>. This model gives fully interval networks that are close analogues to the bacteria‚Äìphage model of <span class="citation" data-cites="Weitz2005CoeArm">Weitz et al. (2005)</span>, with both a modular structure and a non-uniform degree distribution. This model is easy to learn: when trained with features <span class="math inline">[<em>v</em><sub><em>i</em></sub>,‚ÄÜ<em>h</em><sub><em>j</em></sub>,‚ÄÜabs(<em>v</em><sub><em>i</em></sub>,‚ÄÜ<em>h</em><sub><em>j</em></sub>)]<sup><em>T</em></sup></span> to predict the interactions between <span class="math inline"><em>i</em></span> and <span class="math inline"><em>j</em></span>, all four models presented below were able to reach almost perfect predictions all the time (data not presented here) ‚Äì this is in part because the rule is fixed for all interactions. In order to make the problem more difficult to solve, we use <span class="math inline">[<em>v</em><sub><em>i</em></sub>,‚ÄÜ<em>h</em><sub><em>j</em></sub>]</span> as a feature vector (<em>i.e.</em> the traits on which the models are trained), and therefore the models will have to uncover that the rule for interaction is <span class="math inline">abs(<em>v</em><sub><em>i</em></sub>,‚ÄÜ<em>h</em><sub><em>j</em></sub>)‚ÄÑ‚â§‚ÄÑ<em>Œæ</em></span>.</p>
            <p>The training sample is composed of 30% of the <span class="math inline">10<sup>4</sup></span> possible entries in the network, <em>i.e.</em> <span class="math inline"><em>n</em>‚ÄÑ=‚ÄÑ3000</span>. Out of these interactions, we pick a proportion <span class="math inline"><em>ŒΩ</em></span> (the training set bias) to be positive, so that the training set has <span class="math inline"><em>ŒΩ</em><em>n</em></span> interactions, and <span class="math inline">(1‚ÄÖ‚àí‚ÄÖ<em>ŒΩ</em>)<em>n</em></span> non-interactions. We vary <span class="math inline"><em>ŒΩ</em></span> uniformly in <span class="math inline">]0,‚ÄÜ1[</span>. This allows to evaluate how the measures of binary classification performance respond to artificially rebalanced dataset for a given network connectance. The rest of the dataset (<span class="math inline"><em>n</em>‚ÄÑ=‚ÄÑ7000</span> pairs of species) is used as a testing set, on which all furher measures are calculated. Note that although the training set is balanced, the testing set is not, and retains (part of) the imbalance of the original data.</p>
            <p>The dataset used for numerical experiments is composed of 64000 such <span class="math inline">(<em>Œæ</em>,‚ÄÜ<em>ŒΩ</em>)</span> pairs, on which four machines are trained: a decision tree regressor, a boosted regression tree, a ridge regressor, and a random forest regressor. All models were taken from the <code>MLJ.jl</code> package <span class="citation" data-cites="Blaom2020MljJul Blaom2020FleMod">(Blaom et al., 2020; Blaom &amp; Vollmer, 2020)</span> in Julia 1.7 <span class="citation" data-cites="Bezanson2017JulFre">(Bezanson et al., 2017)</span>. All machines use the default parameterization; this is an obvious deviation from best practices, as the hyperparameters of any machine require training before its application on a real dataset. As we use 64000 such datasets, this would require 256000 unique instances of tweaking the hyperparameters, which is not realistic. Therefore, we assume that the default parameterizations are comparable across networks. All machines return a quantitative prediction, usually (but not necessarily) in <span class="math inline">[0,‚ÄÜ1]</span>, which is proportional (but not necessarily linearly) to the probability of an interaction between <span class="math inline"><em>i</em></span> and <span class="math inline"><em>j</em></span>.</p>
            <p>In order to pick the best adjacency matrix for a given trained machine, we performed a thresholding approach using 500 steps on predictions from the testing set, and picking the threshold that maximized Youden‚Äôs informedness, which is usually the optimized target for imbalanced classification. During the thresholding step, we measured the area under the receiving-operator characteristic (ROC-AUC) and precision-recall (PR-AUC) curves, as measures of overall performance over the range of returned values. We report the ROC-AUC and PR-AUC, as well as a suite of other measures as introduced in the next section, for the best threshold. The ensemble model was generated by summing the predictions of all component models on the testing set (ranged in <span class="math inline">[0,‚ÄÜ1]</span>), then put through the same thresholding process. The complete code to run the simulations is given as an appendix; running the final simulation required 4.8 core days (approx. 117 hours).</p>
            <p>After the simulations were completed, we removed all runs (<em>i.e.</em> pairs of <span class="math inline"><em>Œæ</em></span> and <span class="math inline"><em>ŒΩ</em></span>) for which at least one of the following conditions was met: the accuracy was 0, the true positive or true negative rates were 0, the connectance was larger than 0.25. This removes both the obviously failed model runs, and the networks that are more densely connected compared to the connectance of empirical food webs (and are therefore less difficult to predict, being less imbalanced; preliminary analyses of data with a connectance larger than 3 revealed that all machines reached consistently high performance).</p>
            <h2 id="effect-of-training-set-bias-on-performance">Effect of training set bias on performance</h2>
            <p>In fig.¬†3, we present the response of MCC and informedness to (i) five levels of network connectance and (ii) a gradient of training set bias, for the four component models as well as the ensemble. All models reached a higher performance on more connected networks, and using more biased training sets (with the exception of ridge regression, whose informedness decreased in performance with training set bias). In all cases, informedness was extremely high, which is an expected consequence of the fact that this is the value we optimized to determine the cutoff. MCC increased with training set bias, although this increase became less steep with increasing connectance. Interestingly, the ensemble almost always outclassed its component models. In a few cases, both MCC and informedness stared decreasing when the training set bias got too close to one, which suggests that it is possible to over-correct the imbalance.</p>
            <figure>
            <img src="figures/bias_mcc_inf.png" id="fig:biasmccinf" alt="Figure 3: Response of MCC and Informedness to changes in the training set bias for a fixed connectance (rows). Both of these values approach 1 for a good model. Informedness is consistently high, and by contrast, MCC increases with additional training set bias. Across all models, training on a more connected network is easier." /><figcaption aria-hidden="true">Figure 3: Response of MCC and Informedness to changes in the training set bias for a fixed connectance (rows). Both of these values approach 1 for a good model. Informedness is consistently high, and by contrast, MCC increases with additional training set bias. Across all models, training on a more connected network is easier.</figcaption>
            </figure>
            <p>In fig.¬†4, we present the same information as fig.¬†3, this time using ROC-AUC and PR-AUC. ROC-AUC is always high, and does not vary with training set bias. On the other hand, PR-AUC shows very strong responses, increasing with training set bias. It is notable here that two classifiers that seemed to be performing well (Decision Tree and Random Forest) based on their MCC are not able to reach a high PR-AUC even at higher connectances. As in fig.¬†3, the ensemble outperforms its component models.</p>
            <figure>
            <img src="figures/bias_roc_pr.png" id="fig:biasrocpr" alt="Figure 4: Response of ROC-AUC and PR-AUC to changes in the training set bias for a fixed connectance (rows). ROC-AUC is consistently high, and therefore not properly able to separate good from poor classifiers. On the other hand, PR-AUC responds to changes in the training set. As in fig.¬†3, training on more connected networks is easier." /><figcaption aria-hidden="true">Figure 4: Response of ROC-AUC and PR-AUC to changes in the training set bias for a fixed connectance (rows). ROC-AUC is consistently high, and therefore not properly able to separate good from poor classifiers. On the other hand, PR-AUC responds to changes in the training set. As in fig.¬†3, training on more connected networks is easier.</figcaption>
            </figure>
            <p>Based on the results presented in fig.¬†3 and fig.¬†4, it seems that informedness and ROC-AUC are not necessarily able to discriminate between good and bad classifiers (although this result may be an artifact for informedness, as it has been optimized when thresholding). On the other hand, MCC and PR-AUC show a strong response to training set bias, and may therefore be more useful at model comparison.</p>
            <h2 id="required-amount-of-positives-to-get-the-best-performance">Required amount of positives to get the best performance</h2>
            <p>The previous results revealed that the measure of classification performance responds both to the bias in the training set <em>and</em> to the connectance of the network; from a practical point of view, assembling a training set requires to withold positive information, which in ecological networks are very scarce (and typically more valuable than negatives, on which there is a doubt). For this reason, across all values of connectance, we measured the training set bias that maximized a series of performance measures. When this value is high, the training set needs to skew more positive in order to get a performant model; when this value is about 0.5, the training set needs to be artificially balanced to optimize the model performance. These results are presented in fig.¬†5.</p>
            <figure>
            <img src="figures/optim_bias.png" id="fig:optimbias" alt="Figure 5: Value of the optimal training set bias for the different models and measures evaluated here, over a range of connectances. Informedness was reliably maximized for balanced training sets, and kept this behavior across models. For other measures, larger connectances in the true network allowed lower biases in the training set. In a large number of cases, ‚Äúover-correcting‚Äù by having training sets with more than half instances representing interactions would maximize the values of the model performance measures." /><figcaption aria-hidden="true">Figure 5: Value of the optimal training set bias for the different models and measures evaluated here, over a range of connectances. Informedness was reliably maximized for balanced training sets, and kept this behavior across models. For other measures, larger connectances in the true network allowed lower biases in the training set. In a large number of cases, ‚Äúover-correcting‚Äù by having training sets with more than half instances representing interactions would maximize the values of the model performance measures.</figcaption>
            </figure>
            <p>The more ‚Äúoptimistic‚Äù measures (ROC-AUC and informedness) required a biasing of the dataset from about 0.4 to 0.75 to be maximized, with the amount of bias required decreasing only slightly with the connectance of the original network. MCC and PR-AUC required values of training set bias from 0.75 to almost 1 to be optimized, which is in line with the results of the previous section, <em>i.e.</em> they are more stringent tests of model performance. These results suggest that learning from a dataset with very low connectance can be a different task than for more connected networks: it becomes increasingly important to caputre the mechanisms that make an interaction <em>exist</em>, and therefore having a slightly more biased training dataset might be beneficial. As connectance increases, the need for biased training sets is less prominent, as learning the rules for which interactions <em>do not</em> exist starts gaining importance.</p>
            <figure>
            <img src="figures/optim_perf.png" id="fig:optimperf" alt="Figure 6: When trained on their optimally biased training set, most models were able to maximize their performance; this is not true for decision tree, which had a very low PR-AUC, and to some extent for ridge regression who had a slow increase with network connectance. The ensemble had a consistently high performance despite incorporating poor models." /><figcaption aria-hidden="true">Figure 6: When trained on their optimally biased training set, most models were able to maximize their performance; this is not true for decision tree, which had a very low PR-AUC, and to some extent for ridge regression who had a slow increase with network connectance. The ensemble had a consistently high performance despite incorporating poor models.</figcaption>
            </figure>
            <p>When trained at their optimal training set bias, connectance still had a significant impact on the performance of some machines fig.¬†6. Notably, Decision Tree, Random Forest, and Ridge Regression had low values of PR-AUC. In all cases, the Boosted Regression Tree was reaching very good predictions (esepcially for connectances larger than 0.1), and the ensemble was almost always scoring perfectly. This suggests that all the models are biased in different ways, and that the averaging in the ensemble is able to correct these biases. We do not expect this last result to have any generality, and provide a discussion of a recent exemple in which the ensemble was performing worse than its components models.</p>
            <h1 id="do-better-classification-accuracy-result-in-more-realistic-networks">Do better classification accuracy result in more realistic networks?</h1>
            <p>In this last section, we generate a network using the same model as before, with <span class="math inline"><em>S</em><sub>1</sub>,‚ÄÜ<em>S</em><sub>2</sub>‚ÄÑ=‚ÄÑ50,‚ÄÜ80</span> species, a connectance of <span class="math inline">‚ÄÑ‚âà‚ÄÑ0.16</span> (<span class="math inline"><em>Œæ</em>‚ÄÑ=‚ÄÑ0.19</span>), and a training set bias of <span class="math inline">0.7</span>. The prediction made on the complete dataset is presented in fig.¬†7. Visualizing the results this way highlights the importance of exploratory data analysis: whereas all models return a network with interactions laying mostly on the diagonal (as expected), the Ridge Regression is quite obviously biased. Despite this, we can see that the ensemble is close to the initial dataset.</p>
            <figure>
            <img src="figures/valid_ensemble.png" id="fig:ecovalid" alt="Figure 7: Visualisation of the models predictions for one instance of a network prediction problem (shown in the ‚ÄúDataset‚Äù panel). This figure reveals how inspecting the details of the prediction is important: indeed, although the performance measures hint at the fact that ridge regression is mediocre, this figure reveals that it is making predictions that correspond to a network with an entirely different topology (namely, nested as opposed to diagonal)." /><figcaption aria-hidden="true">Figure 7: Visualisation of the models predictions for one instance of a network prediction problem (shown in the ‚ÄúDataset‚Äù panel). This figure reveals how inspecting the details of the prediction is important: indeed, although the performance measures hint at the fact that ridge regression is mediocre, this figure reveals that it is making predictions that correspond to a network with an entirely different topology (namely, nested as opposed to diagonal).</figcaption>
            </figure>
            <p>The trained models were then thresholded (again by optimising informedness), and their predictions transformed back into networks for analysis; specifically, we measured the connectance, nestedness (REF), and modularity (REF). This process was repeated 250 times, and the results are presented in tbl.¬†1. The random forest model is an interesting instance here: it produces the network that looks the most like the original dataset, despite having a very low PR-AUC, suggesting it hits high recall at the cost of low precision. Although the ensemble was able to reach a very high PR-AUC (and a very high ROC-AUC), this did not necessarily translate into more accurate reconstructions of the structure of the network. This result bears elaborating. Measures of model performance capture how much of the interactions and non-interactions are correctly identified. As long as these predictions are not perfect, some interactions will be predicted at the ‚Äúwrong‚Äù position in the network; these measures cannot describe the structural effect of these mistakes. On the other hand, measures of network structure can have the same value with interactions that fall at drastically different positions; this is in part because a lot of these measures covary with connectance, and in part because as long as these values are not 0 or their respective maximum, there is a large number of network configurations that can have the same value. That ROC-AUC is consistently larger than PR-AUC may be a case of this measure masking models that are not, individually, strong predictors <span class="citation" data-cites="Jeni2013FacImb">(Jeni et al., 2013)</span>.</p>
            <div id="tbl:comparison">
            <table>
            <caption>Table 1: Values of four performance metrics, and three network structure metrics, for 250 independent predictions similar to the ones presented in fig.¬†7. The values in <strong>bold</strong> indicate the best value for each column (including ties). Because the values have been rounded, values of 1.0 for the ROC-AUC column indicate an average <span class="math inline">‚ÄÑ‚â•‚ÄÑ0.99</span>.</caption>
            <thead>
            <tr class="header">
            <th style="text-align: right;">Model</th>
            <th style="text-align: center;">MCC</th>
            <th style="text-align: center;">Inf.</th>
            <th style="text-align: center;">ROC-AUC</th>
            <th style="text-align: center;">PR-AUC</th>
            <th style="text-align: center;">Conn.</th>
            <th style="text-align: center;"><span class="math inline"><em>Œ∑</em></span></th>
            <th style="text-align: center;"><span class="math inline"><em>Q</em></span></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td style="text-align: right;">Decision tree</td>
            <td style="text-align: center;">0.85</td>
            <td style="text-align: center;">0.92</td>
            <td style="text-align: center;">0.97</td>
            <td style="text-align: center;">0.12</td>
            <td style="text-align: center;">0.21</td>
            <td style="text-align: center;">0.76</td>
            <td style="text-align: center;">0.31</td>
            </tr>
            <tr class="even">
            <td style="text-align: right;">BRT</td>
            <td style="text-align: center;"><strong>0.90</strong></td>
            <td style="text-align: center;">0.90</td>
            <td style="text-align: center;">0.98</td>
            <td style="text-align: center;">0.86</td>
            <td style="text-align: center;">0.23</td>
            <td style="text-align: center;">0.82</td>
            <td style="text-align: center;">0.27</td>
            </tr>
            <tr class="odd">
            <td style="text-align: right;">Random Forest</td>
            <td style="text-align: center;"><strong>0.90</strong></td>
            <td style="text-align: center;"><strong>0.96</strong></td>
            <td style="text-align: center;"><strong>1.00</strong></td>
            <td style="text-align: center;">0.27</td>
            <td style="text-align: center;"><strong>0.20</strong></td>
            <td style="text-align: center;"><strong>0.72</strong></td>
            <td style="text-align: center;"><strong>0.32</strong></td>
            </tr>
            <tr class="even">
            <td style="text-align: right;">Ridge Regression</td>
            <td style="text-align: center;">0.80</td>
            <td style="text-align: center;">0.91</td>
            <td style="text-align: center;">0.95</td>
            <td style="text-align: center;">0.58</td>
            <td style="text-align: center;">0.24</td>
            <td style="text-align: center;">1.0</td>
            <td style="text-align: center;">0.18</td>
            </tr>
            <tr class="odd">
            <td style="text-align: right;">Ensemble</td>
            <td style="text-align: center;">0.88</td>
            <td style="text-align: center;">0.94</td>
            <td style="text-align: center;"><strong>1.00</strong></td>
            <td style="text-align: center;"><strong>0.96</strong></td>
            <td style="text-align: center;"><strong>0.20</strong></td>
            <td style="text-align: center;">0.75</td>
            <td style="text-align: center;">0.31</td>
            </tr>
            <tr class="even">
            <td style="text-align: right;">Data</td>
            <td style="text-align: center;"></td>
            <td style="text-align: center;"></td>
            <td style="text-align: center;"></td>
            <td style="text-align: center;"></td>
            <td style="text-align: center;">0.18</td>
            <td style="text-align: center;">0.66</td>
            <td style="text-align: center;">0.34</td>
            </tr>
            </tbody>
            </table>
            </div>
            <h1 id="guidelines-for-the-assesment-of-network-predictive-models">Guidelines for the assesment of network predictive models</h1>
            <p>The results presented here highlight an interesting paradox: although the Random Forest was ultimately able to get a correct estimate of network structure tbl.¬†1, it ultimately remains a poor classifier, as evidenced by its low PR-AUC. This suggests that the goal of predicting <em>interactions</em> and predicting <em>networks</em> may not be solvable in the same way ‚Äì of course a perfect classifier of interactions would make a perfect network prediction; but even the best scoring predictor of interactions (the ensemble model) had not necessarily the best prediction of network structure. The tasks of predicting networks structure and of predicting interactions within networks are essentially two different ones. For some applications (<em>.e.g.</em> comparison of network structure across gradients), one may care more about a robust estimate of the structure, at the cost at putting some interactions at the wrong place. For other applications (<em>e.g.</em> identifying pairs of interacting species), one may conversely care more about getting as many pairs right, even though the mistakes accumulate in the form of a slightly worse estimate of network structure. How these two approaches can be reconciled is undoubtedly a task for further research. Despite this apparent tension at the heart of the predictive exercise, we can use the results presented here to suggest a number of guidelines.</p>
            <p>First, because we should have more trust in reported interactions than in reported absences of interactions, we can draw on previous literature to recommend informedness as a measure to decide on a threshold <span class="citation" data-cites="Chicco2021MatCor">(Chicco et al., 2021)</span>; this being said, because informedness is insensitive to bias, the model performance is better evaluated through the use of MCC fig.¬†3. Because <span class="math inline"><em>F</em><sub>1</sub></span> is monotonously sensitive to classifier bias fig.¬†1 and network connectance fig.¬†2, MCC should be prefered as a measure of model evaluation.</p>
            <p>Second, because the PR-AUC responds more to network connectance fig.¬†6 and training set imbalance fig.¬†4, it should be used as a measure of model performance over the ROC-AUC. This is not to say that ROC-AUC should be discarded (in fact, a low ROC-AUC is a sign of an issue with the model), but that its interpretation should be guided by the PR-AUC value. Specifically, a high ROC-AUC is not informative, as it can be associated to a low PR-AUC (see <em>e.g.</em> Random Forest in tbl.¬†1) This again echoes recommendations from other fields <span class="citation" data-cites="Saito2015PrePlo Jeni2013FacImb">(Jeni et al., 2013; Saito &amp; Rehmsmeier, 2015)</span>.</p>
            <p>Thirdly, regardless of network connectance, maximizing informedness required a training set bias of about 0.5, and maximizing the MCC required a training set bias of 0.7 and more. This has an important consequence in ecological networks, for which the pool of positive cases (interactions) to draw from is typically small: the most parsimonious measure (<em>i.e.</em> the one requiring to discard the least amount of information to train the model) will give the best validation potential, and is probably the informedness <span class="citation" data-cites="Schisterman2005OptCut">(maximizing informedness is the generally accepted default for imbalanced classification; Schisterman et al., 2005)</span>.</p>
            <p>Finally, it is noteworthy that the ensemble model was systematically better than the component models; even when the models were individually far form perfect, the ensemble was able to leverage the different biases expressed by the models to make an overall more accurate prediction. We do not expect that ensembles will <em>always</em> be better than single models. In a recent multi-model comparison, <span class="citation" data-cites="Becker2021OptPre">Becker et al. (2021)</span> found that the ensemble was <em>not</em> the best model. There is no general conclusion to draw from this besides reinforcing the need to be pragmatic about which models should be included in the ensemble, or whether to use an ensemble at all. In a sense, the surprising peformance of the ensemble model should form the basis of the last recommendation: optimal training set bias and its interaction with connectance and binary classifier is, in a sense, an hyperparameter that should be assessed. The distribution of results in fig.¬†5 and fig.¬†6 show that there are variations around the trend; furthermore, networks with different structures than the one we simulated here may respond in different ways.</p>
            <p><strong>Acknowledgements:</strong> We acknowledge that this study was conducted on land within the traditional unceded territory of the Saint Lawrence Iroquoian, Anishinabewaki, Mohawk, Huron-Wendat, and Om√†miwininiwak nations. We thank Colin J. Carlson, Michael D. Catchen, Giulio Valentino Dalla Riva, and Tanya Strydom for inputs on earlier versions of this manuscript. This research was enabled in part by support provided by Calcul Qu√©bec (www.calculquebec.ca) and Compute Canada (www.computecanada.ca) through the Narval general purpose cluster. TP is supported by a NSERC Discovery Grant and Discovery Acceleration Supplement, by funding to the Viral Emergence Research Initiative (VERENA) consortium including NSF BII 2021909, and by a grant from the Institut de Valorisation des Donn√©es (IVADO).</p>
            <h1 class="unnumbered" id="references">References</h1>
            <div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="doc-bibliography">
            <div id="ref-Allouche2006AssAcc" class="csl-entry" role="doc-biblioentry">
            Allouche, O., Tsoar, A., &amp; Kadmon, R. (2006). Assessing the accuracy of species distribution models: Prevalence, kappa and the true skill statistic (TSS). <em>Journal of Applied Ecology</em>, <em>43</em>(6), 1223‚Äì1232. <a href="https://doi.org/10.1111/j.1365-2664.2006.01214.x">https://doi.org/10.1111/j.1365-2664.2006.01214.x</a>
            </div>
            <div id="ref-Becker2021OptPre" class="csl-entry" role="doc-biblioentry">
            Becker, D., Albery, G. F., Sjodin, A. R., Poisot, T., Bergner, L., Dallas, T., Eskew, E. A., Farrell, M. J., Guth, S., Han, B. A., Simmons, N. B., Stock, M., Teeling, E. C., &amp; Carlson, C. J. (2021). Optimizing predictive models to prioritize viral discovery in zoonotic reservoirs. <em>bioRxiv</em>, 2020.05.22.111344. <a href="https://doi.org/10.1101/2020.05.22.111344">https://doi.org/10.1101/2020.05.22.111344</a>
            </div>
            <div id="ref-Bezanson2017JulFre" class="csl-entry" role="doc-biblioentry">
            Bezanson, J., Edelman, A., Karpinski, S., &amp; Shah, V. (2017). Julia: A Fresh Approach to Numerical Computing. <em>SIAM Review</em>, <em>59</em>(1), 65‚Äì98. <a href="https://doi.org/10.1137/141000671">https://doi.org/10.1137/141000671</a>
            </div>
            <div id="ref-Blaom2020MljJul" class="csl-entry" role="doc-biblioentry">
            Blaom, A. D., Kiraly, F., Lienart, T., Simillides, Y., Arenas, D., &amp; Vollmer, S. J. (2020). MLJ: A Julia package for composable machine learning. <em>Journal of Open Source Software</em>, <em>5</em>(55), 2704. <a href="https://doi.org/10.21105/joss.02704">https://doi.org/10.21105/joss.02704</a>
            </div>
            <div id="ref-Blaom2020FleMod" class="csl-entry" role="doc-biblioentry">
            Blaom, A. D., &amp; Vollmer, S. J. (2020, December 31). <em>Flexible model composition in machine learning and its implementation in MLJ</em>. <a href="http://arxiv.org/abs/2012.15505">http://arxiv.org/abs/2012.15505</a>
            </div>
            <div id="ref-Boughorbel2017OptCla" class="csl-entry" role="doc-biblioentry">
            Boughorbel, S., Jarray, F., &amp; El-Anbari, M. (2017). Optimal classifier for imbalanced data using Matthews Correlation Coefficient metric. <em>PloS One</em>, <em>12</em>(6), e0177678. <a href="https://doi.org/10.1371/journal.pone.0177678">https://doi.org/10.1371/journal.pone.0177678</a>
            </div>
            <div id="ref-Branco2015SurPre" class="csl-entry" role="doc-biblioentry">
            Branco, P., Torgo, L., &amp; Ribeiro, R. (2015, May 13). <em>A Survey of Predictive Modelling under Imbalanced Distributions</em>. <a href="http://arxiv.org/abs/1505.01658">http://arxiv.org/abs/1505.01658</a>
            </div>
            <div id="ref-Chicco2020AdvMat" class="csl-entry" role="doc-biblioentry">
            Chicco, D., &amp; Jurman, G. (2020). The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. <em>BMC Genomics</em>, <em>21</em>(1), 6. <a href="https://doi.org/10.1186/s12864-019-6413-7">https://doi.org/10.1186/s12864-019-6413-7</a>
            </div>
            <div id="ref-Chicco2021MatCor" class="csl-entry" role="doc-biblioentry">
            Chicco, D., T√∂tsch, N., &amp; Jurman, G. (2021). The Matthews correlation coefficient (MCC) is more reliable than balanced accuracy, bookmaker informedness, and markedness in two-class confusion matrix evaluation. <em>BioData Mining</em>, <em>14</em>, 13. <a href="https://doi.org/10.1186/s13040-021-00244-z">https://doi.org/10.1186/s13040-021-00244-z</a>
            </div>
            <div id="ref-Delgado2019WhyCoh" class="csl-entry" role="doc-biblioentry">
            Delgado, R., &amp; Tibau, X.-A. (2019). Why Cohen‚Äôs Kappa should be avoided as performance measure in classification. <em>PloS One</em>, <em>14</em>(9), e0222916. <a href="https://doi.org/10.1371/journal.pone.0222916">https://doi.org/10.1371/journal.pone.0222916</a>
            </div>
            <div id="ref-Ferri2009ExpCom" class="csl-entry" role="doc-biblioentry">
            Ferri, C., Hern√°ndez-Orallo, J., &amp; Modroiu, R. (2009). An experimental comparison of performance measures for classification. <em>Pattern Recognition Letters</em>, <em>30</em>(1), 27‚Äì38. <a href="https://doi.org/10.1016/j.patrec.2008.08.010">https://doi.org/10.1016/j.patrec.2008.08.010</a>
            </div>
            <div id="ref-He2013ImbLea" class="csl-entry" role="doc-biblioentry">
            He, H., &amp; Ma, Y. (Eds.). (2013). <em>Imbalanced Learning: Foundations, Algorithms, and Applications</em> (1st edition). Wiley-IEEE Press.
            </div>
            <div id="ref-Jeni2013FacImb" class="csl-entry" role="doc-biblioentry">
            Jeni, L. A., Cohn, J. F., &amp; De La Torre, F. (2013). Facing Imbalanced Data‚ÄìRecommendations for the Use of Performance Metrics. <em>2013 Humaine Association Conference on Affective Computing and Intelligent Interaction</em>, 245‚Äì251. <a href="https://doi.org/10.1109/ACII.2013.47">https://doi.org/10.1109/ACII.2013.47</a>
            </div>
            <div id="ref-Jordano2016ChaEco" class="csl-entry" role="doc-biblioentry">
            Jordano, P. (2016a). Chasing Ecological Interactions. <em>PLOS Biol</em>, <em>14</em>(9), e1002559. <a href="https://doi.org/10.1371/journal.pbio.1002559">https://doi.org/10.1371/journal.pbio.1002559</a>
            </div>
            <div id="ref-Jordano2016SamNet" class="csl-entry" role="doc-biblioentry">
            Jordano, P. (2016b). Sampling networks of ecological interactions. <em>Functional Ecology</em>. <a href="https://doi.org/10.1111/1365-2435.12763">https://doi.org/10.1111/1365-2435.12763</a>
            </div>
            <div id="ref-Landis1977MeaObs" class="csl-entry" role="doc-biblioentry">
            Landis, J. R., &amp; Koch, G. G. (1977). The Measurement of Observer Agreement for Categorical Data. <em>Biometrics</em>, <em>33</em>(1), 159‚Äì174. <a href="https://doi.org/10.2307/2529310">https://doi.org/10.2307/2529310</a>
            </div>
            <div id="ref-MacDonald2020RevLin" class="csl-entry" role="doc-biblioentry">
            MacDonald, A. A. M., Banville, F., &amp; Poisot, T. (2020). Revisiting the Links-Species Scaling Relationship in Food Webs. <em>Patterns</em>, <em>1</em>(0). <a href="https://doi.org/10.1016/j.patter.2020.100079">https://doi.org/10.1016/j.patter.2020.100079</a>
            </div>
            <div id="ref-McLeod2021SamAsy" class="csl-entry" role="doc-biblioentry">
            McLeod, A., Leroux, S. J., Gravel, D., Chu, C., Cirtwill, A. R., Fortin, M.-J., Galiana, N., Poisot, T., &amp; Wood, S. A. (2021). Sampling and asymptotic network properties of spatial multi-trophic networks. <em>Oikos</em>, <em>n/a</em>(n/a). <a href="https://doi.org/10.1111/oik.08650">https://doi.org/10.1111/oik.08650</a>
            </div>
            <div id="ref-Poisot2021ImpMam" class="csl-entry" role="doc-biblioentry">
            Poisot, T., Ouellet, M.-A., Mollentze, N., Farrell, M. J., Becker, D. J., Albery, G. F., Gibb, R. J., Seifert, S. N., &amp; Carlson, C. J. (2021, May 31). <em>Imputing the mammalian virome with linear filtering and singular value decomposition</em>. <a href="http://arxiv.org/abs/2105.14973">http://arxiv.org/abs/2105.14973</a>
            </div>
            <div id="ref-Saito2015PrePlo" class="csl-entry" role="doc-biblioentry">
            Saito, T., &amp; Rehmsmeier, M. (2015). The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets. <em>PLOS ONE</em>, <em>10</em>(3), e0118432. <a href="https://doi.org/10.1371/journal.pone.0118432">https://doi.org/10.1371/journal.pone.0118432</a>
            </div>
            <div id="ref-Schisterman2005OptCut" class="csl-entry" role="doc-biblioentry">
            Schisterman, E. F., Perkins, N. J., Liu, A., &amp; Bondell, H. (2005). Optimal Cut-point and Its Corresponding Youden Index to Discriminate Individuals Using Pooled Blood Samples. <em>Epidemiology</em>, <em>16</em>(1), 73‚Äì81. <a href="https://doi.org/10.1097/01.ede.0000147512.81966.ba">https://doi.org/10.1097/01.ede.0000147512.81966.ba</a>
            </div>
            <div id="ref-Somodi2017PreDep" class="csl-entry" role="doc-biblioentry">
            Somodi, I., Lepesi, N., &amp; Botta‚ÄêDuk√°t, Z. (2017). Prevalence dependence in model goodness measures with special emphasis on true skill statistics. <em>Ecology and Evolution</em>, <em>7</em>(3), 863‚Äì872. <a href="https://doi.org/10.1002/ece3.2654">https://doi.org/10.1002/ece3.2654</a>
            </div>
            <div id="ref-Strydom2021RoaPre" class="csl-entry" role="doc-biblioentry">
            Strydom, T., Catchen, M. D., Banville, F., Caron, D., Dansereau, G., Desjardins-Proulx, P., Forero-Mu√±oz, N. R., Higino, G., Mercier, B., Gonzalez, A., Gravel, D., Pollock, L., &amp; Poisot, T. (2021). A roadmap towards predicting species interaction networks (across space and time). <em>Philosophical Transactions of the Royal Society B: Biological Sciences</em>, <em>376</em>(1837), 20210063. <a href="https://doi.org/10.1098/rstb.2021.0063">https://doi.org/10.1098/rstb.2021.0063</a>
            </div>
            <div id="ref-Weitz2005CoeArm" class="csl-entry" role="doc-biblioentry">
            Weitz, J. S., Hartman, H., &amp; Levin, S. A. (2005). Coevolutionary arms races between bacteria and bacteriophage. <em>Proceedings of the National Academy of Sciences of the United States of America</em>, <em>102</em>(27), 9535‚Äì9540. <a href="https://doi.org/10.1073/pnas.0504062102">https://doi.org/10.1073/pnas.0504062102</a>
            </div>
            <div id="ref-Whalen2021NavPit" class="csl-entry" role="doc-biblioentry">
            Whalen, S., Schreiber, J., Noble, W. S., &amp; Pollard, K. S. (2021). Navigating the pitfalls of applying machine learning in genomics. <em>Nature Reviews Genetics</em>, 1‚Äì13. <a href="https://doi.org/10.1038/s41576-021-00434-9">https://doi.org/10.1038/s41576-021-00434-9</a>
            </div>
            <div id="ref-Youden1950IndRat" class="csl-entry" role="doc-biblioentry">
            Youden, W. J. (1950). Index for rating diagnostic tests. <em>Cancer</em>, <em>3</em>(1), 32‚Äì35. <a href="https://doi.org/10.1002/1097-0142(1950)3:1&lt;32::AID-CNCR2820030106&gt;3.0.CO;2-3">https://doi.org/10.1002/1097-0142(1950)3:1&lt;32::AID-CNCR2820030106&gt;3.0.CO;2-3</a>
            </div>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <h2>Affiliations</h2> 
            <sup>1</sup>&nbsp;Universit√© de Montr√©al; <sup>2</sup>&nbsp;Qu√©bec Centre for Biodiversity Sciences
            <br />
            
        </div>
    </footer>
    
</body>
</html>