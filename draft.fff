\begin{figure}
\hypertarget{fig:bias}{%
\centering
\includegraphics{figures/changing-bias.png}
\caption{Consequences of changing the classifier skills (\(s\)) and bias
(\(s\)) for a connectance \(\rho=0.15\), on accuracy, \(F_1\), postive
predictive value, and \(\kappa\). Accuracy increases with skill, but
also increases when the bias tends towards estimating \emph{fewer}
interactions. The \(F_1\) score increases with skill but also increases
when the bias tends towards estimating \emph{more} interactions; PPV
behaves in the same way. Interestingly, \(\kappa\) responds as expected
to skill (being negative whenever \(s < 0.5\)), and peaks for values of
\(b \approx 0.5\); nevertheless, the value of bias for which \(\kappa\)
is maximized in \emph{not} \(b=0.5\), but instead increases with
classifier skill. In other words, at equal skill, maximizing \(\kappa\)
would lead to select a \emph{more} biased classifier.}\label{fig:bias}
}
\end{figure}
\efloatseparator
 
\begin{figure}
\hypertarget{fig:connectance}{%
\centering
\includegraphics{figures/changing-connectance.png}
\caption{As in fig.~\ref{fig:bias}, consequences of changing connectance
for different levels of classifier skill, assuming no classifier bias.
Informedness, \(\kappa\), and MCC do increase with connectance, but only
when the classifier is not no-skill; by way of contrast, a more
connected network will give a higher \(F_1\) value even with a no-skill
classifier.}\label{fig:connectance}
}
\end{figure}
\efloatseparator
 
\begin{figure}
\hypertarget{fig:biasco}{%
\centering
\includegraphics{figures/bias_by_connectance.png}
\caption{Response of MCC, Informedness, ROC-AUC, and PR-AUC to changes
in the training set balance (on the \(x\) axis) for a series of
increasing connectances (color). All of these values approach 1 for a
good model, but should be lower when the prediction is more difficult.
Informedness is consistently high, and by contrast, MCC increases with
additional training set balance. Across all models, training on a more
connected network is easier. ROC-AUC is consistently high, and therefore
not properly able to separate good from poor classifiers. On the other
hand, PR-AUC responds to changes in the training set.}\label{fig:biasco}
}
\end{figure}
\efloatseparator
 
\begin{figure}
\hypertarget{fig:optimbias}{%
\centering
\includegraphics{figures/optim_bias.png}
\caption{Value of the optimal training set balance for the different
models and measures evaluated here, over a range of connectances.
Informedness was reliably maximized for balanced training sets, and kept
this behavior across models. For other measures, larger connectances in
the true network allowed lower biases in the training set. In a large
number of cases, ``over-correcting'' by having training sets with more
than half instances representing interactions would maximize the values
of the model performance measures.}\label{fig:optimbias}
}
\end{figure}
\efloatseparator
 
\begin{figure}
\hypertarget{fig:optimvalue}{%
\centering
\includegraphics{figures/optim_value.png}
\caption{When trained on their optimally biased training set, most
models were able to maximize their performance; this is not true for
decision tree, which had a very low PR-AUC, and to some extent for ridge
regression who had a slow increase with network connectance. The
ensemble had a consistently high performance despite incorporating poor
models.}\label{fig:optimvalue}
}
\end{figure}
\efloatseparator
 
\begin{figure}
\hypertarget{fig:ecovalid}{%
\centering
\includegraphics{figures/valid_ensemble.png}
\caption{Visualisation of the models predictions for one instance of a
network prediction problem (shown in the ``Dataset'' panel). This figure
reveals how inspecting the details of the prediction is important:
indeed, although the performance measures hint at the fact that ridge
regression is mediocre, this figure reveals that it is making
predictions that correspond to a network with an entirely different
topology (namely, nested as opposed to diagonal).}\label{fig:ecovalid}
}
\end{figure}
