using Statistics
using StatsFuns
using StatsBase
using DataFramesMeta
using Random
using Statistics
using MLJ
using DataFrames
using CSV: CSV
using Distributions
using AlgebraOfGraphics, CairoMakie
using EcologicalNetworks
using Random

Random.seed!(1)

# AUC
function âˆ«(x::Array{T}, y::Array{T}) where {T<:Number}
    S = zero(Float64)
    for i in 2:length(x)
        S += (x[i] - x[i - 1]) * (y[i] + y[i - 1]) * 0.5
    end
    return .-S
end

# Confusion matrix utilities
include(joinpath(@__DIR__, "confusionmatrix.jl"))

# Network generation function
function L(x::T, y::T; r::T=0.1) where {T<:Number}
    return (x - r / 2.0) â‰¤ y â‰¤ (x + r / 2.0) ? one(T) : zero(T)
end
function network(S, Î¾)
    infectivity = Beta(6.0, 8.0)
    resistance = Beta(2.0, 8.0)
    ð±áµ¥ = sort(rand(infectivity, S[1]))
    ð±â‚• = sort(rand(resistance, S[2]))
    ð±â‚ = repeat(ð±áµ¥; outer=length(ð±â‚•))
    ð±â‚‚ = repeat(ð±â‚•; inner=length(ð±áµ¥))
    ð±â‚ƒ = abs.(ð±â‚ .- ð±â‚‚)
    ð² = [L(ð±â‚[i], ð±â‚‚[i]; r=Î¾) for i in 1:length(ð±â‚)]
    #ð± = table(hcat(ð±â‚, ð±â‚‚, ð±â‚ƒ))
    ð± = table(hcat(ð±â‚, ð±â‚‚))
    return (ð±, ð²)
end

function R(x, m, M)
    x = x .- m
    x = x ./ (M - m)
    return x
end

R(x) = R(x, extrema(x)...)


DecisionTree = @load DecisionTreeRegressor pkg = DecisionTree
RandomForest = @load RandomForestRegressor pkg = DecisionTree
BoostedRegressor = @load EvoTreeRegressor pkg = EvoTrees
RidgeRegressor = @load RidgeRegressor pkg = MLJLinearModels

candidate_models = [
    :DecTree => DecisionTree(),
    :BRT => BoostedRegressor(),
    :RF => RandomForest(),
    :RR => RidgeRegressor(),
]

S = (50,80)

results = DataFrame(
    run = Int64[],
    model = Symbol[],
    measure = Symbol[],
    value = Float64[]
)

for i in 1:250

    ð—, ð² = network(S, 0.19)

    net = BipartiteNetwork(reshape(Bool.(ð²), S))
    push!(results, (i, :data, :Connectance, connectance(net)))
    push!(results, (i, :data, :Nestedness, Î·(net)))
    push!(results, (i, :data, :Modularity, Q(brim(lp(net)...)...)))

    bias = 0.7

    training_size = round(Int64, 0.3 * length(ð²))
    n_positive = round(Int64, training_size * bias)
    idx_pos = sample(findall(iszero.(ð²)), n_positive; replace=true)
    idx_neg = sample(findall(isone.(ð²)), training_size - n_positive; replace=true)
    Iâ‚š = shuffle(vcat(idx_neg, idx_pos))
    Iâ‚’ = setdiff(eachindex(ð²), Iâ‚š)

    m = Pair[] # Predictions (raw)
    Ms = [] # Best confmat

    for candidate_model in candidate_models
        this_machine = machine(candidate_model.second, ð—, ð²)
        fit!(this_machine; rows=Iâ‚š)
        prediction = mean.(MLJ.predict(this_machine; rows=Iâ‚’))

        # The prediction isn't clamped because we threshold it so who gives a shit
        thresholds = LinRange(minimum(prediction), maximum(prediction), 500)

        # Confusion matrix
        binobs = Bool.(ð²[Iâ‚’])
        M = Vector{ConfusionMatrix}(undef, length(thresholds))
        for (i, Ï„) in enumerate(thresholds)
            binpred = prediction .>= Ï„
            tp = sum(binobs .& binpred)
            tn = sum(.!binobs .& .!binpred)
            fp = sum(.!binobs .& binpred)
            fn = sum(binobs .& .!binpred)
            M[i] = ConfusionMatrix(tp, tn, fp, fn)
        end
        ROCAUC = âˆ«(fpr.(M), tpr.(M))
        AUPRC = âˆ«(tpr.(M), ppv.(M))
        ðŒ = M[last(findmax(informedness.(M)))]

        push!(results, (i, candidate_model.first, :ROCAUC, ROCAUC))
        push!(results, (i, candidate_model.first, :PRAUC, AUPRC))
        push!(results, (i, candidate_model.first, :MCC, mcc(ðŒ)))
        push!(results, (i, candidate_model.first, :INF, informedness(ðŒ)))

        push!(Ms, ðŒ)
        push!(m, this_machine => thresholds[last(findmax(informedness.(M)))])

        # Network
        pr = MLJ.predict(this_machine)
        thr = thresholds[last(findmax(informedness.(M)))]
        net = BipartiteNetwork(reshape(pr .>= thr, S))
        push!(results, (i, candidate_model.first, :Connectance, connectance(net)))
        push!(results, (i, candidate_model.first, :Nestedness, Î·(net)))
        push!(results, (i, candidate_model.first, :Modularity, Q(brim(lp(net)...)...)))

    end


    # Ensemble and thresholding
    ens_pred = mean(hcat([R(MLJ.predict(m[j].first)) for j in 1:length(m)]...); dims=2)
    binobs = Bool.(ð²[Iâ‚’])
    thresholds = LinRange(minimum(ens_pred), maximum(ens_pred), 500)
    M = Vector{ConfusionMatrix}(undef, length(thresholds))
    for (i, Ï„) in enumerate(thresholds)
        binpred = ens_pred[Iâ‚’] .>= Ï„
        tp = sum(binobs .& binpred)
        tn = sum(.!binobs .& .!binpred)
        fp = sum(.!binobs .& binpred)
        fn = sum(binobs .& .!binpred)
        M[i] = ConfusionMatrix(tp, tn, fp, fn)
    end
    ROCAUC = âˆ«(fpr.(M), tpr.(M))
    AUPRC = âˆ«(tpr.(M), ppv.(M))
    ðŒ = M[last(findmax(informedness.(M)))]
    thr = thresholds[last(findmax(informedness.(M)))]
    push!(Ms, ðŒ)
    ens_thres = thresholds[last(findmax(informedness.(M)))]
    push!(results, (i, :ensemble, :ROCAUC, ROCAUC))
    push!(results, (i, :ensemble, :PRAUC, AUPRC))
    push!(results, (i, :ensemble, :MCC, mcc(ðŒ)))
    push!(results, (i, :ensemble, :INF, informedness(ðŒ)))

    net = BipartiteNetwork(reshape(ens_pred .>= thr, S))
    push!(results, (i, :ensemble, :Connectance, connectance(net)))
    push!(results, (i, :ensemble, :Nestedness, Î·(net)))
    push!(results, (i, :ensemble, :Modularity, Q(brim(lp(net)...)...)))

end

allowmissing!(results, :value)
replace!(results.value, NaN => missing)
dropmissing!(results)
bmm = groupby(results, [:model, :measure])
opt = combine(bmm, :value => (x -> round(mean(x); digits=2)) => :mean)
unstack(opt, :model, :measure, :mean)